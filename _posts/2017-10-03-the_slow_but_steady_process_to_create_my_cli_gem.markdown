---
layout: post
title:      "The Slow but Steady Process to Create my CLI Gem"
date:       2017-10-03 17:08:34 -0400
permalink:  the_slow_but_steady_process_to_create_my_cli_gem
---


The project took longer than I anticipated, but I am finally (finally!) finished with my CLI gem. I encountered a few delays along the way, both in the context of my code and in the world outside of the Learn program, but I’ve now completed the assignment. My application has a CLI that controls the behavior of the program, scraper methods that obtain data from a website, and multiple objects that manage and perform functions on that information. 

When deciding what type of CLI application I wanted to create, I considered a few different themes that interested me and then settled on one that seemed straightforward. As I (and my roommate) have pets, and I have a growing collection of houseplants, I frequently visit the ASPCA website to learn about the toxicity of various plants. With that in mind, I decided to create a CLI that provides the same details with only a couple of keyboard commands.

Based on how the Poisonous Plants section of the ASPCA’s website is structured, I decided that my CLI would prompt the user to enter a letter of the alphabet and then provide an alphabetical list of the plants with names that start with that letter. Then the user could select a plant in that list in order to see more details about it, or they could specify a different letter and view another list. So, aside from a CLI class, I determined that my program would need List and Plant classes.

These classes are responsible for handling the various list contents and plant details. When a new List instance is called, it initializes with a @letter value and an array of Plant objects, which are initialized concurrently with @name and @url properties. When the user requests more information about a specific plant, additional details are obtained based on the data available on its web page. These details are assigned to the remaining instance variables for that plant (as applicable): @other_names, @scientific_name, @toxicity, @non_toxicity, and @clinical_signs.  I elected to omit a few of the site’s details that didn’t seem relevant to my application, namely the plant’s family and its toxic principles (if applicable). 

To check that my objects interacted correctly, I assigned hard-coded values to each instance so that I could include actual output in my CLI. Once I was able to display the lists and plants as desired, and select specific options based on user input, I was ready to obtain real data from the relevant web pages, and my plan was to create a separate Scraper class. Obviously, I had to start by finding the correct selectors for my scraper methods. That ended up being the most difficult aspect of my entire application. I found the ASPCA website to be a difficult data source, probably due in large part to my minimal experience with scraping.

On the website, the default view for plants is alphabetical by letter, but only 15 plants are listed on each page. Therefore, I had to determine the selectors for each plant listed and then find the selectors that would make it possible to navigate to the remaining pages, where more yet plants would be selected. Needless to say, it took a lot of guessing and checking, and there were many versions of my scraping methods throughout this project. Even when I thought I was finished, there were a couple of instances when certain portions had to be amended to account for exceptions that I discovered, which either caused inaccurate lists to be displayed or resulted in errors when I ran my code. For example, “X” has no plants to list, and “C” has a 10th page, containing a single plant, that I didn’t notice at first because the link is not visible until you reach the 5th page. After checking and re-checking, though, I’m confident that my scraper methods are capable of gathering the relevant details from the web pages, or at least the current versions of them. The code is perhaps a bit more complicated than I expected, but it works.

Once I had the code for my scraper methods, I still had to determine when and where to call them. After much back and forth, I decided to make them class methods that would be used exclusively in the List class. As I structured my objects in a List “has many” Plants manner, and the List class was already responsible for obtaining each plant’s name and url upon initialization, it seemed that the List instance should also be responsible for getting the remaining details to finalize its Plant objects.

Finally, I had all of the components that my application needed, so I worked on restructuring some of the content and refactoring the existing code. One thing I decided to do was move all output responsibilities to the CLI. My original method stubs were scattered across the various classes, but I decided that content would be displayed through the CLI, while specific responsibilities that contributed to the CLI methods were left in each class. For example, each plant has a #details method that orders its instance variables for being displayed and also weeds out those with no values (based on what is included on the ASPCA site). This method returns the array of variables that is used by the CLI when displaying the plant’s details.

Another part of creating the application that required some research and lots of different attempts was the “behind the scenes” portion: file dependencies, Gemspec file contents, etc. To be honest, that was an aspect of the project that I found daunting at the beginning, as I’m not very familiar with those types of programming details yet. However, the various resources provided with the lesson were very helpful, and I do feel like I understand more about such details after working on this project. This is something I plan to work on more, too, because otherwise the code itself is rather useless!

Overall, as I wrap up this project, I feel more confident in my ability to create an application, from start to finish, and prepare it for public use. It was difficult, but I’m satisfied with the results. My code is certainly not perfect, but I’m proud of it, simply because I feel like it represents a milestone in my coding education. Actually, I think any student who completes this assignment should feel accomplished at the end of it. It’s evidence that they understand how to build a CLI, utilize object-oriented programming, and scrape data from the web, which I think are very significant, useful skills that can be built upon throughout the student’s programming education and beyond. For me, at least, it seems like my application represents many key coding concepts that I’m understanding more and adding to with each new lesson I finish. Now that I’m, at last, ready to submit it, I’m eager to see what comes next.

